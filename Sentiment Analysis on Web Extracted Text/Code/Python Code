#!/usr/bin/env python
# coding: utf-8

# # 1.	Extract reviews of any product from e-commerce website Amazon.

# In[41]:


import requests   # Importing requests to extract content from a url
from bs4 import BeautifulSoup as bs # Beautifulsoup is for web scrapping...used to scrap specific content 
import re 


# In[42]:


import matplotlib.pyplot as plt
from wordcloud import WordCloud


# In[43]:


# creating empty reviews list 
saree_reviews=[]


for i in range(1,150):
  ip=[]  
  url="https://www.amazon.in/Devangi-Fashion-pure-Saree-DF_Patola_106_Red/product-reviews/B07YFHJ92P/ref=cm_cr_othr_d_show_all_btm?ie=UTF8&reviewerType=all_reviews"+str(i)
  response = requests.get(url)
  soup = bs(response.content,"html.parser")# creating soup object to iterate over the extracted content 
  reviews = soup.find_all("span",attrs={"class","a-size-base review-text review-text-content"})# Extracting the content under specific tags  
  for i in range(len(reviews)):
    ip.append(reviews[i].text)  
 
  saree_reviews=saree_reviews+ip  # adding the reviews of one page to empty list which in future contains all the reviews


# In[44]:


# writng reviews in a text file 
with open("saree.txt","w",encoding='utf8') as output:
    output.write(str(saree_reviews))


# In[45]:


# Joinining all the reviews into single paragraph 
ip_rev_string = " ".join(saree_reviews)
ip_rev_string


# In[46]:


import nltk
# from nltk.corpus import stopwords


# In[47]:


# Removing unwanted symbols incase if exists
ip_rev_string = re.sub("[^A-Za-z" "]+"," ", ip_rev_string).lower()
ip_rev_string = re.sub("[0-9" "]+"," ", ip_rev_string)


# In[48]:


# words that contained in iphone XR reviews
ip_reviews_words = ip_rev_string.split(" ")


# In[49]:


# TFIDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 3))
X = vectorizer.fit_transform(ip_reviews_words)


# In[50]:


with open(r"C:\Users\Yogesh Thakur\Sentiment-Analysis(NLP)\Stopwords.txt","r") as sw:
    stop_words = sw.read()
    
stop_words = stop_words.split("\n")


# In[51]:


#stop_words.extend(["oneplus","mobile","time","android","phone","device","screen","battery","product","good","day","price"])


# In[52]:


ip_reviews_words = [w for w in ip_reviews_words if not w in stop_words]


# In[53]:


# Joinining all the reviews into single paragraph 
ip_rev_string = " ".join(ip_reviews_words)
ip_rev_string


# In[54]:


# WordCloud can be performed on the string inputs.
# Corpus level word cloud

wordcloud_ip = WordCloud(
                      background_color='White',
                      width=1800,
                      height=1400
                     ).generate(ip_rev_string)

plt.imshow(wordcloud_ip)


# In[70]:


# Open the text file containing positive words
with open(r"C:\Users\Yogesh Thakur\Sentiment-Analysis(NLP)\Positive Words.txt", "r") as file:
    positive_words = [line.strip() for line in file]

#print("Content of ip_reviews_words:", ip_reviews_words)
#print("Content of poswords:", poswords)


# In[71]:


# Convert the list of positive words to lowercase
positive_words = [word.lower() for word in positive_words]
# Print the list of positive words
print("Positive Words:", positive_words)


# In[ ]:





# In[72]:


# Filter positive words from ip_reviews_wordspositive_words
ip_pos_in_pos = " ".join([w for w in ip_reviews_words if w in positive_words])
print("Filtered positive words:", ip_pos_in_pos)


# In[73]:


wordcloud_pos_in_pos = WordCloud(
                      background_color='White',
                      width=1800,
                      height=1400
                     ).generate(ip_pos_in_pos)
plt.figure(2)
plt.imshow(wordcloud_pos_in_pos)


# In[76]:


# negative words Choose path for -ve words stored in system
with open(r"C:\Users\Yogesh Thakur\Sentiment-Analysis(NLP)\Negative Words.txt", "r") as neg:
  negwords = neg.read().split("\n")


# In[77]:


# negative word cloud
# Choosing the only words which are present in negwords
ip_neg_in_neg = " ".join ([w for w in ip_reviews_words if w in negwords])


# In[78]:


wordcloud_neg_in_neg = WordCloud(
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(ip_neg_in_neg)
plt.figure(3)
plt.imshow(wordcloud_neg_in_neg)


# In[80]:


# wordcloud with bigram
nltk.download('punkt')
from wordcloud import WordCloud, STOPWORDS


WNL = nltk.WordNetLemmatizer()
WNL


# In[83]:


# Lowercase and tokenize
text = ip_rev_string.lower()


# In[84]:


# Remove single quote early since it causes problems with the tokenizer.
text = text.replace("'", "")


# In[85]:


tokens = nltk.word_tokenize(text)
text1 = nltk.Text(tokens)
print(tokens)
print(text1)


# In[86]:


# Remove extra chars and remove stop words.
text_content = [''.join(re.split("[ .,;:!?‘’``''@#$%^_&*()<>{}~\n\t\\\-]", word)) for word in text1]


# In[87]:


# Create a set of stopwords
stopwords_wc = set(STOPWORDS)
#customised_words = ['price', 'great'] # If you want to remove any particular word form text which does not contribute much in meaning


# In[88]:


#new_stopwords = stopwords_wc.union(customised_words)

# Remove stop words
text_content = [word for word in text_content if word not in stopwords_wc]


# In[89]:


# Take only non-empty entries
text_content = [s for s in text_content if len(s) != 0]


# In[90]:


# Best to get the lemmas of each word to reduce the number of similar words
text_content = [WNL.lemmatize(t) for t in text_content]


# In[91]:


nltk_tokens = nltk.word_tokenize(text)  
bigrams_list = list(nltk.bigrams(text_content))
print(bigrams_list)


# In[92]:


dictionary2 = [' '.join(tup) for tup in bigrams_list]
print (dictionary2)


# In[93]:


# Using count vectoriser to view the frequency of bigrams
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(ngram_range=(2, 2))
bag_of_words = vectorizer.fit_transform(dictionary2)
vectorizer.vocabulary_


# In[94]:


sum_words = bag_of_words.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
print(words_freq[:100])


# In[96]:


# Generating wordcloud
words_dict = dict(words_freq)
WC_height = 1000
WC_width = 1500
WC_max_words = 200
wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width, stopwords=stopwords_wc)
wordCloud.generate_from_frequencies(words_dict)


plt.figure(4)
plt.title('Most frequently occurring bigrams connected by same colour and font size')
plt.imshow(wordCloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# In[ ]:




